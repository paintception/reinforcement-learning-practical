\documentclass{beamer}

\usepackage[english]{babel}
\usepackage{xcolor}
\usepackage{xmpmulti}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{multicol}
\usepackage{tikz}
\usepackage{eucal}
\usetikzlibrary{positioning,angles,quotes}
\usepackage{url}
\usepackage{graphicx}
\usepackage{cmbright}
\usepackage{framed}
\usepackage{tabularx}
\usepackage{amssymb}
\usepackage{pifont}

\usetikzlibrary{pgfplots.groupplots,arrows.meta,shadows,positioning,angles,quotes}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{positioning}
\usepackage{pgfplots}

%\input{epgfplotslibrary{groupplots}
\usetikzlibrary{pgfplots.groupplots,arrows.meta,shadows,positioning,angles,quotes}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}

\newcommand{\xmark}{\ding{55}}

\DeclareMathOperator*{\argmax}{arg\,max}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 

\definecolor{Maroon}{cmyk}{0, 0.87, 0.68, 0.32}
\definecolor{RoyalBlue}{cmyk}{1, 0.50, 0, 0}
\definecolor{skymagenta}{rgb}{0.81, 0.44, 0.69}

\newenvironment{takeaway}[1]{%
	\definecolor{shadecolor}{gray}{0.9}%
		\begin{shaded}{\color{skymagenta}\noindent\textsc{#1}}\\%
		}{%
		\end{shaded}%
}



%%%%%% THE FOLLOWING FILE CONTAINS THE STYLE DEFINITIONS %%%%%%
\input{header.tex}
%%%%%%

%%%%%% TITLE, AUTHOR, DATE DEFINITIONS %%%%%%
\title{Beyond Model-Free Reinforcement Learning}
	\subtitle{Solving Optimal Decision Making Problems from a Different Perspective}
\author{Matthia Sabatelli}

\date{\today}
%%%%%%

\setbeamertemplate{footline}[frame number]{}

\begin{document}

\frame{\titlepage} 

\begin{frame}{Recap}

	Last week we have seen ...
	\begin{block}{Model-Free Reinforcement Learning}
		\begin{itemize}
			\item How to construct algorithms when parts of the MDP $\mathcal{M}$ are unknown
			\item Monte-Carlo (MC) Learning
			\item Temporal-Difference (TD) Learning
			\item The concept of bootstrapping
			\item How to learn $Q^{\pi}(s,a)$ in an \textit{on-policy} fashion or in an \textit{off-policy} fashion
		\end{itemize}
	\end{block}

\end{frame}



\frame{\frametitle{Today's Agenda}\tableofcontents}


\begin{frame}{Policy Gradient Methods}
	\section{Beyond Learning a Value Function}
	Throughout this course we have constantly seen how important \textcolor{RoyalBlue}{value functions} are:
	\begin{itemize}
		\item They correspond to the knowledge of the agent
		\item They help us understanding the environment
		\item But most importantly they define the policy $\pi$ that the agent follows
	\end{itemize}

	\bigskip

	$\Rightarrow$ The most \textcolor{skymagenta}{powerful} value function is the state-action value function $Q(s,a)$ since:
	\begin{align*}
		\pi^{*}(s) = \underset{a\in\mathcal{A}}{\argmax} \ Q^{*}(s,a) \ \text{for all} \ s \in \mathcal{S}
	\end{align*}

\end{frame}



\begin{frame}{Policy Gradient Methods}
	Our \textcolor{RoyalBlue}{goal} so far has always been that of learning $Q(s,a)$:
	\begin{enumerate}
		\item We can do this in a tabular fashion 
		\item Or we can generalize this process with a function approximator
		\item We can do this in an \textit{on-policy} or \textit{off-policy} fashion
	\end{enumerate}

	\bigskip

	$\Rightarrow$ \textcolor{RoyalBlue}{First} we learn a value function, and \textcolor{skymagenta}{then} we derive the policy $\pi$
	
	\bigskip
	
	$\Rightarrow$ We have \textcolor{Maroon}{never} seen how to learn $\pi$ directly!
	
\end{frame}


\begin{frame}{Policy Gradient Methods}
	Action value based methods such as Q-Learning, SARSA and $\text{QV}(\lambda)$ are very powerful algorithms but have some important \textcolor{Maroon}{limitations}:
	\begin{enumerate}
		\item Are restricted to discrete action spaces 
		\item Work alongside carefully designed exploration-exploitation strategies
		\item Are unable to learn an optimal policy which is stochastic 
		\item Sometimes learning $\pi(a|s;\theta)$ is easier than learning $Q(s,a;\theta)$
	\end{enumerate}

\end{frame}

\begin{frame}{Policy Gradient Methods}
	There is a family of techniques which tries to learn $\pi(a|s;\theta)$ directly:
	\begin{center}
		\textcolor{skymagenta}{\textbf{Policy Gradient Methods}}
	\end{center}

	\bigskip

	$\Rightarrow$ They learn a \textcolor{RoyalBlue}{parametrized policy} that learns how to select actions without having to consult a value function:
	\begin{align*}
		\pi(a|s;\theta)=\text{Pr}\:\{a_t = a, s_t=s\:;\theta_t=\theta\}
	\end{align*}

	\bigskip 

	The parameters $\theta$ usually correspond to the weights of a neural network

\end{frame}

\begin{frame}{Policy Gradient Methods}
	Training policy gradient methods significantly differs from training a Deep-Q Network:
	
	\bigskip

	$\Rightarrow$ The idea of last week's methods was that of \textcolor{Maroon}{minimizing} a certain quantity called the TD-error:
	\begin{align*}
		\mathcal{L}(\theta) = \mathds{E}_{\langle \cdot \rangle\sim U(D)} \bigg[\big(r_{t} + \gamma \: \underset{a\in \mathcal{A}}{\max}\: Q(s_{t+1}, a; \theta^{-}) - Q(s_{t}, a_{t}; \theta)\big)^{2}\bigg]
	\end{align*}

	
	\bigskip

	$\Rightarrow$ Policy Gradient methods instead seek to \textcolor{Maroon}{maximize} some scalar performance measure $J(\theta)$ and update the parameters via gradient ascent!
	\begin{align*}
		\theta_{t+1} = \theta + \alpha \widehat{\nabla J(\theta_t)}
	\end{align*}

\end{frame}


\begin{frame}{Policy Gradient Methods}
	It is also possible to learn $\pi(a|s;\theta)$ in \textcolor{RoyalBlue}{combination} with a value function!

	\bigskip

	$\Rightarrow$ These algorithms come with the name of \textcolor{skymagenta}{\textbf{Actor-Critic}} methods:
	\begin{itemize}
		\item It can be hard to learn a policy $\pi$ directly
		\item We would like to tell our agent learning who is learning $\pi$ how good its policy is 
		\item To do so we can use the state-value function $V^{\pi}(s)$
	\end{itemize}
\end{frame}


\begin{frame}{Policy Gradient Methods}
	How do Actor-Critic methods intuitively work?
		\begin{center}
			\includegraphics[width=\textwidth]{./Images/a2c}
			\caption{Image courtesy of Van de Wolfshaar (2017)}
		\end{center}
\end{frame}


\begin{frame}{Policy Gradient Methods}
	How do we train this network?

	\begin{align*}
		\theta_{t+1} & = \theta + \alpha \Big(r_t+\gamma V(s_{t+1;\phi})-v(s_t;\phi)\Big)
	\end{align*}

\end{frame}






\begin{frame}{Beyond Value-Based Learning}
	\section{Beyond Value-Based Learning}

\end{frame}


\begin{frame}{Beyond Online Learning}
	\section{Beyond Online Learning}
\end{frame}

\begin{frame}{Beyond Markov Decision Processes}
	\section{Beyond Markov Decision Processes}
\end{frame}





%============================================================================
\end{document}
